# Articles TLDR AI 06-03-2023

It is raining AI-powered features all across search engines.¬† 

Sign Up [https://tldr.tech/ai?utm_source=tldr]|Jobs
[https://tldr.tech/jobs]|Advertise
[https://danni763618.typeform.com/to/K4Gdz1?utm_source=tldrai&utm_medium=newsletter#newsletter=ai]|View
Online
[https://actions.tldrnewsletter.com/web-version?ep=1&lc=041b8714-96a1-11ed-9899-3729ef006681&p=535c3cc4-bbf4-11ed-970d-7f4580e08fac&pt=campaign&t=1678111573&s=fff841be4290af87303650c6be3649c66dde0d5cfcffb85e3973b28ae4cd3119]


		TLDR 

DAILY UPDATE 2023-03-06

üöÄ 

HEADLINES & LAUNCHES

BRAVE SEARCH LAUNCHES AN AI-POWERED SUMMARIZATION FEATURE (3 MINUTE
READ)
[https://techcrunch.com/2023/03/02/brave-search-launches-an-ai-powered-summarization-feature/?utm_source=tldrai]


It is raining AI-powered features all across search engines. Brave
Search launched a new ‚ÄúSummarizer‚Äù feature, which is powered by
different large language models (LLMs) ‚Äî OpenAI‚Äôs GPT tech isn‚Äôt
one of them. Just like the name suggests, its job is to provide a
synopsis of a search query using different sources. The summary
feature is available to all Brave Search users on desktop and mobile
‚Äî accessible through any browser. 

100 MOST CITED AI PAPERS IN 2022 (6 MINUTE READ)
[https://www.zeta-alpha.com/post/must-read-the-100-most-cited-ai-papers-in-2022?utm_source=tldrai]


Google still holds the top spot in AI research and UC Berkeley has
that honor among academic groups. Given the volume of papers from
groups like Google and Deepmind, it is impressive to see much smaller
groups have far fewer but potentially more impactful papers. 

200-YEAR-OLD MATH OPENS UP AI‚ÄôS MYSTERIOUS BLACK BOX (4 MINUTE READ)
[https://spectrum.ieee.org/black-box-ai?utm_source=tldrai] 

Whether it‚Äôs designing microchips or dreaming up new proteins,
sometimes it seems like neural networks can do anything. Infamously,
however, these brain-inspired AI systems work in mysterious ways,
raising concerns that what they are doing might not make any sense.
New research suggests that 200-year-old math could help shed light on
how neural networks perform complex tasks such as predicting climate
or modeling turbulence, a new study finds. This in turn could help
boost the accuracy of neural networks and the speed at which they
learn, researchers say. 

üß† 

RESEARCH & INNOVATION

A BETTER OF UNDERSTANDING OF DIFFUSION THROUGH THE LENS OF ELBO
MAXIMIZATION (22 MINUTE READ)
[https://arxiv.org/abs/2303.00848?utm_source=tldrai] 

Diffusion is currently one of the best generative methods in terms of
sample quality. This comes from the nice properties of the loss and
denoising process. In the community, different weighting schemes are
used for noise per step which leads to different empirical results. It
turns out that these non-uniform weighting schemes can be understood
from the lens of likelihood maximization. This is a great step towards
a deeper theoretical understanding of this modern workhorse. 

GUIDE TEXT GENERATION WITH GROUNDING FOR ROBOTIC CONTROL (7 MINUTE
READ) [https://grounded-decoding.github.io/?utm_source=tldrai] 

Applying large language models to robots is challenging due to their
lack of experience with the physical world. To overcome this, a guided
decoding strategy is used to construct an action sequence that is both
likely according to the language model and realizable in the
environment. This strategy solves complex tasks in a robotic setting
by leveraging the knowledge of both models. They compare their work to
SayCan. 

UNLIMITED-SIZE DIFFUSION RESTORATION (12 MINUTE READ)
[https://arxiv.org/abs/2303.00354v1?utm_source=tldrai] 

This paper discusses the use of diffusion models for zero-shot image
restoration and proposes a method to handle images of arbitrary sizes
while maintaining the excellent characteristics of zero-shot. The
proposed method, called Mask-Shift Restoration, addresses local
incoherence, and Hierarchical Restoration alleviates out-of-domain
issues. These simple, parameter-free approaches can be used not only
for image restoration but also for image generation of unlimited
sizes. 

üßë‚Äçüíª 

ENGINEERING & RESOURCES

INTERESTED IN SPEEDING UP MULTIGPU TRAINING? GRADIENT ACCUMULATION IS
YOUR BOTTLENECK (6 MINUTE READ)
[https://muellerzr.github.io/blog/gradient_accumulation.html?utm_source=tldrai]


Distributed Pytorch can be immensely slow if you‚Äôre not careful.
When running a model on multiple GPUs, you take the average of all the
gradient updates at a predefined interval. However, if you‚Äôre not
careful, your code will be synchronizing across GPUs for _EVERY_
backwards pass. 

EARLY DROPOUT FOR MITIGATING UNDERFITTING IN NEURAL NETWORKS (GITHUB
REPO) [https://github.com/facebookresearch/dropout?utm_source=tldrai] 

The study shows that early dropout, applied only during the initial
phases of training, can mitigate underfitting by reducing the
directional variance of gradients and aligning them with the entire
dataset‚Äôs gradient. The proposed method consistently improves
generalization accuracy in various vision tasks and encourages further
research on regularization in deep learning. 

CONTEXT CLUSTERS: A NEW PARADIGM FOR VISUAL REPRESENTATION (GITHUB
REPO) [https://github.com/ma-xu/context-cluster?utm_source=tldrai] 

This work introduces Context Clusters (CoCs), a new paradigm for
visual representation that views an image as a set of unorganized
points and extracts features via a simplified clustering algorithm.
CoCs are convolution- and attention-free and only rely on clustering
for spatial interaction. Despite not targeting state-of-the-art
performance, CoCs achieve comparable or even better results than
ConvNets or ViTs on several benchmarks. 

üéÅ 

MISCELLANEOUS

THE INSIDE STORY OF HOW CHATGPT WAS BUILT (10 MINUTE READ)
[https://archive.ph/Dk7ka?utm_source=tldrai] 

To get the inside story behind the ChatGPT ‚Äî how it was made, how
OpenAI has been updating it since release, and how its makers feel
about its success ‚Äî the author talked to four people who helped
build it. They are Sandhini Agarwal (works on policy at OpenAI), Liam
Fedus (scientist who worked on ChatGPT), John Schulman (cofounder of
OpenAI), and Jan Leike (leader of OpenAI‚Äôs alignment team) 

IS OPENAI MAKING MONEY OFF CHATGPT API (1 MINUTE READ)
[https://threadreaderapp.com/thread/1631485296754987014.html?utm_source=tldrai]


With some assumptions, it seems like the newest GPT-Turbo model might
still be massively profitable despite the steep drop in price. 

WHY AI WON‚ÄôT CAUSE UNEMPLOYMENT (5 MINUTE READ)
[https://pmarca.substack.com/p/why-ai-wont-cause-unemployment?utm_source=tldrai]


In this article, Marc Andreessen makes the case that similar to other
previously new technologies, AI will not cause mass unemployment. 

‚ö° 

QUICK LINKS

THERE‚ÄôS AN AI FOR THAT (1 MINUTE READ)
[https://theresanaiforthat.com/?utm_source=tldrai] 

An extensive list of many useful AI tools being built today for
various applications. A great place to gather resources or
inspiration. 

OPENAI-PYTHON (GITHUB REPO)
[https://github.com/openai/openai-python?utm_source=tldrai] 

OpenAI-Python provides access to the OpenAI API from applications
written in Python. 

OPENAI PHP (GITHUB REPO)
[https://github.com/openai-php/client?utm_source=tldrai] 

OpenAI PHP is a PHP API client that allows you to interact with the
OpenAI AI API. 

MACHINE LEARNING NOTES (GITHUB REPO)
[https://github.com/rasbt/machine-learning-notes?utm_source=tldrai] 

Machine Learning Notes is a collection of useful machine learning
codes and snippets. 

If you are in a hiring position, you may want to HIRE AI TALENT
THROUGH OUR FREE JOB BOARD [https://tldr.tech/employer/sign-up]. 

If your company is interested in reaching an audience of AI
decision-makers, researchers, and engineers, you may want to ADVERTISE
WITH US
[https://danni763618.typeform.com/to/K4Gdz1?utm_source=tldrai&utm_medium=newsletter#newsletter=ai].


If you have any comments or feedback, just respond to this email! 

Thanks for reading, 
Andrew Tan (@ANDREWZTAN [https://twitter.com/andrewztan]) & Andrew
Carr (@ANDREW_N_CARR [https://twitter.com/andrew_n_carr]) 

If you don't want to receive future editions of TLDR AI, please¬†click
here to unsubscribe
[https://actions.tldrnewsletter.com/unsubscribe?ep=1&l=eedf6b14-3de3-11ed-9a32-0241b9615763&lc=041b8714-96a1-11ed-9899-3729ef006681&p=535c3cc4-bbf4-11ed-970d-7f4580e08fac&pt=campaign&pv=4&spa=1678111219&t=1678111573&s=c93bf12cb7766af4d892a22fde3424529f5b82d3644244fed95371fde69dd5a3].


¬†