# Articles TLDR AI 17-04-2023

Google is feeling the heat from AI competitors like the new Bing, as
Samsung considers making Bing its default search engine.¬† 

Sign Up [https://tldr.tech/ai?utm_source=tldr]|Jobs
[https://danni763618.typeform.com/to/rSL4lOH3]|Advertise
[https://share.hsforms.com/1OxvmrkcFS4qsxKpNXCi76wee466?utm_source=tldrai&utm_medium=newsletter]|View
Online
[https://actions.tldrnewsletter.com/web-version?ep=1&lc=041b8714-96a1-11ed-9899-3729ef006681&p=351d9662-dd4c-11ed-8bcd-0d62eb6b8ed2&pt=campaign&t=1681756098&s=2874a41e95d516aeb24491f87f356d742369c8cd5b8c03689964ce98066b28d0]


		TLDR 

TLDR AI 2023-04-17

üöÄ 

HEADLINES & LAUNCHES

PRINCETON LAUNCHES CENTER FOR LANGUAGE AND INTELLIGENCE (2 MINUTE
READ) [https://climbing-mantis.10web.site/?utm_source=tldrai] 

Recent progress in large language models has been dominated by large
industry players. However, many of the key ideas still come from
academia. Princeton is hoping to continue contributions to language
and intelligence with this new center. They‚Äôre hiring for research
scientists, engineers, and postdoc scholars. 

WORLD'S SIMPLEST TEXT-TO-IMAGE MODEL? (6 MINUTE READ)
[https://laion.ai/blog/paella/?utm_source=tldrai] 

Laion just released the Paella image generation model. It uses a
discrete, token based, latent space and a cross entropy denoising
objective. One goal of this project is to make an image synthetics
model that almost anyone can understand. Their sampling code is only
12 lines long! 

GOOGLE RUSHING TO ADD AI TO SEARCH ENGINE (5 MINUTE READ)
[https://www.nytimes.com/2023/04/16/technology/google-search-engine-ai.html?utm_source=tldrai]


Google is feeling the heat from AI competitors like the new Bing, as
Samsung considers making Bing its default search engine. Google's
search business, which was worth $162 billion last year, could take a
significant hit if Samsung decides to switch. In response, Google is
working on an AI-powered search engine project called Magi to provide
a more personalized experience. While details on the new search
technology are still under wraps, Google has over 160 people working
on the project, with plans to roll out new features to the public in
the coming months. 

üß† 

RESEARCH & INNOVATION

SCALING, EMERGENCE, AND REASONING IN LLMS (6 MINUTE READ)
[https://docs.google.com/presentation/d/1EUV7W7X_w0BDrscDhPg7lMGzJCkeaPkGCJ3bN8dluXc/edit?resourcekey=0-7Nz5A7y8JozyVrnDtcEKJA#slide=id.g16197112905_0_0?utm_source=tldrai]


A presentation given by Jason Wei about how language model performance
changes with scale. It‚Äôs a lovely discussion about emergent
capabilities in these models and what we can do with those new skills.
One interesting take-away is that we haven‚Äôt come close to
exhausting all possible tasks, so there could be a variety of skills
in the model that we haven‚Äôt uncovered. 

SO LONG HYPERPARAMETERS (38 MINUTE READ)
[https://arxiv.org/abs/2304.05187?utm_source=tldrai] 

What if we didn‚Äôt have to tune the pile of hyperparameters we do to
make deep learning work? This is a pretty technical paper that
presents Automatic Gradient Descent which is a new optimizer that
takes into account network architectures and eliminates a set of
previously required hyperparameters. They work through all the math
thoroughly, and the resulting optimizer seems to be fairly simple.
They don‚Äôt have it working yet for all network architectures, but
that feels like only a matter of time. 

DINOV2: LEARNING ROBUST VISUAL FEATURES WITHOUT SUPERVISION (18 MINUTE
READ) [https://arxiv.org/abs/2304.07193?utm_source=tldrai] 

Recent breakthroughs in natural language processing have paved the way
for foundation models in computer vision, producing all-purpose visual
features through self-supervised methods and large, diverse datasets.
By utilizing an automatic pipeline for curated data and training a
1-billion-parameter ViT model, researchers have created smaller models
that surpass existing all-purpose features in most benchmarks at image
and pixel levels. 

üßë‚Äçüíª 

ENGINEERING & RESOURCES

SVMS ARE BETTER THAN KNNS FOR RETRIEVAL (JUPYTER NOTEBOOK)
[https://github.com/karpathy/randomfun/blob/master/knn_vs_svm.ipynb?utm_source=tldrai]


The former head of AI from Tesla Andrej Karpathy has a gift for
teaching difficult concepts. This notebook shows how to use SVMs
instead of KNNs to find matches in an embedding index. There is a lot
of talk around using embeddings to solve hallucination and this is a
concrete example on how to improve performance generally with a
different traditional ML method. 

OPENAI CONSISTENCY MODELS CODE (GITHUB REPO)
[https://github.com/openai/consistency_models?utm_source=tldrai] 

We wrote recently about the new type of genitive model from OpenAI
that is faster than diffusion and more stable than a GAN. They just
released code and model checkpoints on a set of small experimental
data sets. This model is a cool next step in generative image
modeling, and now we can play around with it! 

SPECTFORMER: FREQUENCY AND ATTENTION IS WHAT YOU NEED IN A VISION
TRANSFORMER (3 MINUTE READ)
[https://badripatro.github.io/SpectFormers/?utm_source=tldrai] 

Combining spectral and multi-headed attention layers in the novel
SpectFormer architecture improves transformer performance for image
recognition tasks. This approach achieves state-of-the-art accuracy on
ImageNet-1K and consistently strong results in transfer learning,
object detection, and instance segmentation tasks across various
datasets. 

üéÅ 

MISCELLANEOUS

COST TO TRAIN ANTHROPIC'S NEXT MODEL (4 MINUTE READ)
[https://orenleung.com/anthropic-claude-next-cost?utm_source=tldrai] 

Anthropic recently announced plans to spend a billion dollars on
training a massive language model. They announced this plan before
their strategic partnership with Google, which may offer cheaper
compute via their TPU pods. In any case it seems like one of the
largest expenses of this endeavor is actually AI researcher salaries,
followed closely by compute costs. 

OPENASSISTANT‚ÄôS HUGE CHAT DATASET (HUGGINGFACE DATASET)
[https://huggingface.co/datasets/OpenAssistant/oasst1?utm_source=tldrai]


There are 161,443 messages with well over 400k quality annotations
such as toxicity and quality. This open dataset is intended to help
researchers working on alignment of language models. 

91% OF ML MODELS DEGRADE OVER TIME (6 MINUTE READ)
[https://www.nannyml.com/blog/91-of-ml-perfomance-degrade-in-time?utm_source=tldrai]


A study reveals that 91% of machine learning models experience
performance degradation over time, emphasizing the need for ongoing
monitoring, maintenance, and adaptation to ensure these models remain
effective and relevant in dynamic environments. 

‚ö° 

QUICK LINKS

MARKETING CO-PILOT AI (PRODUCT LAUNCH)
[https://www.producthunt.com/posts/marketing-co-pilot-ai?utm_source=tldrai]


Describe your content style ‚Äî get a personalized list of 60 tweet
ideas. Powered by AI. Generate up to 5 content plans per account. Save
them in PDF or copy them to Notion. 

EUROPEAN PARLIAMENT PREPARES TOUGH AI MEASURES (2 MINUTE READ)
[https://archive.ph/J5fUz?utm_source=tldrai] 

The European parliament is preparing tough new measures over the use
of artificial intelligence, including forcing chatbot makers to reveal
if they use copyrighted material. 

GPT5 IS NOT IN TRAINING YET (1 MINUTE READ)
[https://gizmodo.com/sam-altman-open-ai-chatbot-gpt4-gpt5-1850337299?utm_source=tldrai]


Sam Altman has said that GPT5 is not yet in training. 

GENERATING CODE WITH TESTS (GITHUB REPO)
[https://github.com/microsoft/CodeT/tree/main/CodeT?utm_source=tldrai]


They achieved a new state of the art (already surpassed by Reflexion)
on the challenging Human Eval benchmark by first generating tests then
determining if the code generated passes those tests. 

WOLVERINE (GITHUB REPO)
[https://github.com/biobootloader/wolverine?utm_source=tldrai] 

Wolverine uses GPT-4 to edit your scripts and tell you what went
wrong. 

TLDR TALENT [https://danni763618.typeform.com/to/rSL4lOH3] is our
exclusive community where we help world-class tech talent and get
intros to companies of their choice, along with a number of exciting
startups and tech companies curated by TLDR.

We give you full control of the process, you can specify if you‚Äôre
actively searching or passively interested only if something amazing
comes along. Set your preferred compensation, seniority/title/role,
specific companies (or types of companies) you‚Äôd like to work for
and more. CLICK HERE TO APPLY
[https://danni763618.typeform.com/to/rSL4lOH3].

If your company is interested in reaching an audience of AI
professionals and early adopters, you may want to ADVERTISE WITH US
[https://share.hsforms.com/1OxvmrkcFS4qsxKpNXCi76wee466?utm_source=tldrai&utm_medium=newsletter].


If you have any comments or feedback, just respond to this email! 

Thanks for reading, 
Andrew Tan [https://twitter.com/andrewztan] & Andrew Carr
[https://twitter.com/andrew_n_carr] 

If you don't want to receive future editions of TLDR AI, please¬†click
here to unsubscribe
[https://actions.tldrnewsletter.com/unsubscribe?ep=1&l=eedf6b14-3de3-11ed-9a32-0241b9615763&lc=041b8714-96a1-11ed-9899-3729ef006681&p=351d9662-dd4c-11ed-8bcd-0d62eb6b8ed2&pt=campaign&pv=4&spa=1681755671&t=1681756098&s=7204498fa7255968164d771b1a1416673330586bb8ac3cee01d6a3a92cf12664].


¬†