# Articles TLDR AI 02-01-2025

Nvidia acquired AI firm Run:ai for an estimated $700 million and plans
to open-source its software. This move aims to enhance GPU cloud
orchestration ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌  ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ 


 Sign Up [1] |Advertise [2]|View Online [3] 

		TLDR 

TLDR AI 2025-01-02

🚀 

HEADLINES & LAUNCHES

 NVIDIA TO OPEN-SOURCE RUN:AI, THE SOFTWARE IT ACQUIRED FOR $700M TO
HELP COMPANIES MANAGE GPUS FOR AI (5 MINUTE READ) [4] 

 Nvidia acquired AI orchestration firm Run:ai for an estimated $700
million and plans to open-source its software. This move aims to
enhance GPU cloud orchestration, offering more flexibility and
efficiency for AI infrastructures. Nvidia's acquisition aligns with
its strategy to broaden its AI ecosystem and address antitrust
concerns. 

 DEEPSEEK: THE QUIET GIANT LEADING CHINA'S AI RACE (26 MINUTE READ)
[5] 

 Deepseek, a Chinese AI startup backed by the hedge fund High-Flyer,
has gained attention by outperforming OpenAI on reasoning benchmarks
and initiating price wars with its efficient AI models. Led by CEO
Liang Wenfeng, Deepseek prioritizes open-source foundational
technology and leverages extensive computing resources without
external funding. The startup focuses on AGI research, challenging
prevailing innovation norms in China while attracting top domestic
talent. 

 HOW OPENAI PLANS TO MOVE FROM BEING A NONPROFIT (8 MINUTE READ) [6] 

 Sam Altman is working to shift control of OpenAI from its founding
nonprofit to a for-profit structure to better compete with tech
giants. Discussions involve setting fair compensation for the
nonprofit and accommodating external stakeholders like Microsoft.
OpenAI faces a deadline to restructure within two years or risk
converting recent investments into debt. 

🧠 

RESEARCH & INNOVATION

 DEEP METRIC LEARNING (27 MINUTE READ) [7] 

 GCA-HNG is a framework for generating more effective hard negatives
by considering global sample correlations instead of just local ones. 

 STATIC QUANTIZATION FOR LLMS (19 MINUTE READ) [8] 

 PrefixQuant is a new method that improves LLM quantization by
isolating outlier tokens offline, eliminating the need for costly
per-token dynamic quantization. 

 EVALUATING MORAL JUDGMENTS IN LLMS (29 MINUTE READ) [9] 

 A new study assesses the ethical decision-making of 51 LLMs in
autonomous driving scenarios, analyzing alignment with human moral
judgments across various models, including GPT, Claude, and Llama. 

🧑‍💻 

ENGINEERING & RESOURCES

 XMODEL-LM (GITHUB REPO) [10] 

 Xmodel-LM is a compact and efficient 1.1B language model pre-trained
on around 2 trillion tokens. Trained on a self-built dataset (Xdata),
which balances Chinese and English corpora based on downstream task
optimization, Xmodel-LM exhibits remarkable performance despite its
smaller size. It notably surpasses existing open-source language
models of similar scale. 

 SAE INTERPRETABILITY TOOL (GITHUB REPO) [11] 

 Automatically figuring out what Sparse Autoencoder features mean can
be challenging. This tool aims to help with some clever clustering and
probability management algorithms. 

 SELF-ASSESSED GENERATION (GITHUB REPO) [12] 

 SAG is a self-supervised framework designed to improve the
generalization of optical flow and stereo methods in real-world
applications. Unlike traditional approaches, SAG uses advanced
reconstruction techniques to generate datasets from RGB images. It
quantifies the confidence level of these results to address
imperfections. 

🎁 

MISCELLANEOUS

 YOUTUBE TEAMS WITH CAA TO LET TALENT IDENTIFY — AND PULL DOWN —
AI DEEPFAKES OF THEMSELVES (4 MINUTE READ) [13] 

 YouTube and CAA are partnering on a program to help talent manage
AI-generated fakes on the platform using early-stage likeness
management technology. This tool will let actors and athletes identify
unauthorized AI replicas and submit removal requests. The
collaboration aims to protect IP rights while testing and refining AI
detection systems before a broader rollout. 

 NEW LLM OPTIMIZATION TECHNIQUE SLASHES MEMORY COSTS UP TO 75% (5
MINUTE READ) [14] 

 Sakana AI's "universal transformer memory" technique optimizes
language models by using neural attention memory models to reduce
unnecessary tokens, improving efficiency and performance. This
approach cuts compute costs, facilitates faster processing, and proves
beneficial across various tasks using open-source models. Researchers
demonstrated significant memory savings and performance enhancements,
especially in handling long sequences for text, code, and multi-modal
tasks. 

 OLYMPUS: BENCHMARKING AI CREATIVITY (6 MINUTE READ) [15] 

 Olympus provides a comprehensive framework for evaluating AI
creativity across multiple domains, offering insights into generative
model capabilities and limitations. 

⚡ 

QUICK LINKS

 ENGINEERED ARTS RESTRUCTURES WITH $10M TO CREATE HUMANOID ROBOTS (4
MINUTE READ) [16] 

 Engineered Arts has restructured as a U.S. company and raised $10
million to expand its humanoid robot operations, bringing total
funding to $16.2 million. 

 ENHANCE NON-IDEAL CT IMAGING (GITHUB REPO) [17] 

 TAMP is a multi-scale integrated Transformer model designed to
enhance non-ideal CT (NICT) imaging. 

 NVIDIA UNVEILS ITS MOST AFFORDABLE GENERATIVE AI SUPERCOMPUTER (3
MINUTE READ) [18] 

 NVIDIA's Jetson Orin Nano Super Developer Kit boosts generative AI
performance, with a new model priced at $249. 

Love TLDR? Tell your friends and get rewards!

 Share your referral link below with friends to get free TLDR swag! 

 https://refer.tldr.tech/34c90d5b/2 [19] 

		 Track your referrals here. [20] 

Want to advertise in TLDR? 📰

 If your company is interested in reaching an audience of AI
professionals and decision makers, you may want to ADVERTISE WITH US
[21]. 

 If you have any comments or feedback, just respond to this email! 

Thanks for reading, 
Andrew Tan & Andrew Carr 

If you don't want to receive future editions of TLDR AI, please
unsubscribe from TLDR AI [22] or manage all of your TLDR newsletter
subscriptions [23]. 

 

Links:
------
[1] https://tldr.tech/ai?utm_source=tldrai
[2] https://advertise.tldr.tech/?utm_source=tldrai&utm_medium=newsletter&utm_campaign=advertisetopnav
[3] https://a.tldrnewsletter.com/web-version?ep=1&lc=041b8714-96a1-11ed-9899-3729ef006681&p=6d121a12-c8ef-11ef-9ce5-35ffb376e9a6&pt=campaign&t=1735826819&s=a7b6c9b1c5240172db41f8a5e44535d71e35466bbd66689b44e3a487a16b8193
[4] https://links.tldrnewsletter.com/35X025
[5] https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas?utm_source=tldrai
[6] https://links.tldrnewsletter.com/q0bFAi
[7] https://arxiv.org/abs/2411.13145v1?utm_source=tldrai
[8] https://arxiv.org/abs/2410.05265v1?utm_source=tldrai
[9] https://arxiv.org/abs/2411.06790v1?utm_source=tldrai
[10] https://github.com/XiaoduoAILab/XmodelLM?utm_source=tldrai
[11] https://github.com/mwatkins1970/SAE_Feature_Interpretability_Tool?utm_source=tldrai
[12] https://github.com/hanlingsgjk/unifiedgeneralization?utm_source=tldrai
[13] https://variety.com/2024/digital/news/caa-youtube-talent-ai-deepfakes-remove-1236251470/?utm_source=tldrai
[14] https://links.tldrnewsletter.com/8NIXYK
[15] https://links.tldrnewsletter.com/DpemFC
[16] https://links.tldrnewsletter.com/7iPmD4
[17] https://github.com/yutinghe-list/tamp?utm_source=tldrai
[18] https://blogs.nvidia.com/blog/jetson-generative-ai-supercomputer/?utm_source=tldrai
[19] https://refer.tldr.tech/34c90d5b/2
[20] https://hub.sparklp.co/sub_46c6316534f5/2
[21] https://advertise.tldr.tech/?utm_source=tldrai&utm_medium=newsletter&utm_campaign=advertisecta
[22] https://a.tldrnewsletter.com/unsubscribe?ep=1&l=eedf6b14-3de3-11ed-9a32-0241b9615763&lc=041b8714-96a1-11ed-9899-3729ef006681&p=6d121a12-c8ef-11ef-9ce5-35ffb376e9a6&pt=campaign&pv=4&spa=1735826437&t=1735826819&s=6371a7cc21c58e32917ea1d344f8df5c206ed85ac823a96c83c22483282f10a3
[23] https://tldr.tech/ai/manage?email=blockchaincryptologue%40gmail.com