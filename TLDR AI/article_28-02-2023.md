# Articles TLDR AI 28-02-2023

ChatLLaMA was introduced as the first open source implementation of
LLaMA¬† 

Sign Up [https://tldr.tech/ai?utm_source=tldr]|Jobs
[https://tldr.tech/jobs]|Advertise
[https://danni763618.typeform.com/to/VdUfrHpY?utm_source=newsletter]|View
Online
[https://actions.tldrnewsletter.com/web-version?ep=1&lc=041b8714-96a1-11ed-9899-3729ef006681&p=0fb8cba2-b732-11ed-81a6-33c00cd07e37&pt=campaign&t=1677593153&s=fad6639e2d4a30c9fbf80975a8ec19cb68554f75f694097c2f6481d7a03da4e1]


		TLDR 

DAILY UPDATE 2023-02-28

üöÄ 

HEADLINES & LAUNCHES

VOICEMOD RAISES $14.5M TO RIDE THE GENERATIVE AI (SONIC)BOOM (5 MINUTE
READ) [https://techcrunch.com/2023/02/24/voicemod/?utm_source=tldrai] 

Voicemod has become the leading creator in real-time voice changing
and soundboard technology. Their mission is to enable everyone to
express themselves through sound. They‚Äôve built expressive and
immersive audio tools, making it easy to create unique sonic
identities and enable interactions with personalized sounds. This will
be huge as the podcast, voiceover, and audiobook spaces have blown up
the last few years. 

DEEP GRAPH LIBRARY REACHES 1.0 (6 MINUTE READ)
[https://www.dgl.ai/release/2023/02/20/release.html?utm_source=tldrai]


A powerful and useful tool in graph deep learning, DGL is now at
version 1.0! Included are hundreds of examples of state of the art
graph networks, baselines, and various graph editing utilities. They
also have modular building blocks for message passing algorithms and
Multi-GPU training. This all combines into a tool kit that can scale
powerful algorithms to graph billions of connections. 

GRAIN AI (PRODUCT LAUNCH) [https://grain.com/ai?utm_source=tldrai] 

Grain is the easiest meeting insights tool to help you understand and
communicate the needs of customers. Anybody in a customer-focused role
can use Grain to record, transcribe, and clip moments from research
interviews, sales calls, and customer meetings. They recently launched
Grain AI, a tool that helps users unlock more value from their
meetings with AI meeting summaries, quick meeting insights, and
shareable customer insights. 

üß† 

RESEARCH & INNOVATION

GENERATE CODE BY RETRIEVING DOCS (31 MINUTE READ)
[https://arxiv.org/abs/2207.05987?utm_source=tldrai] 

With code interfaces regularly changing and the limitation of
in-context learning, there is a strong need to be able to update
program synthesis performance without expensive data collection and
model retraining. This work suggests that using documentation can
improve the generated code of CodeT5. The results are good with ~3%
improvement Pass@1. This isn‚Äôt a foundational change, but may be a
useful trick for practitioners. 

LANGUAGE IS NOT ALL YOU NEED: ALIGNING PERCEPTION WITH LANGUAGE MODELS
(20 MINUTE READ) [https://arxiv.org/abs/2302.14045?utm_source=tldrai] 

KOSMOS-12 is a Multimodal Large Language Model that can learn in
context, follow instructions, and perceive general modalities. It
achieves impressive performance on a range of tasks, including
language understanding, perception-language, and vision tasks. The
model was trained on web-scale multimodal corpora and benefits from
cross-modal transfer. The authors also introduce a dataset for
diagnosing the nonverbal reasoning capability of MLLMs. 

DIRECTED DIFFUSION: DIRECT CONTROL OF OBJECT PLACEMENT THROUGH
ATTENTION GUIDANCE (12 MINUTE READ)
[https://arxiv.org/abs/2302.13153?utm_source=tldrai] 

Text-guided diffusion models struggle to compose scenes with multiple
objects in specific positions, which is crucial in storytelling. To
address this, the authors propose Directed Diffusion, a method that
provides positional control over multiple objects by injecting
"activation" at desired positions in cross-attention maps while
attenuating the rest. This method can be used with an existing
pre-trained model and requires only a few lines of implementation. 

üßë‚Äçüíª 

ENGINEERING & RESOURCES

CHATLLAMA: CHATGPT BASED ON META‚ÄôS LLAMA MODELS (GITHUB REPO)
[https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama?utm_source=tldrai]


Meta has recently released the LLaMA collection, consisting of 7 to 65
billion parameter models that are smaller than GPT-3 but show better
performance. This new collection opens the door to faster inference
performance and real-time assistants, while being cost-effective and
running on a single GPU. However, they were not fine-tuned for
instruction tasks. To address this, ChatLLaMA was introduced as the
first open source implementation of LLaMA based on the Reinforcement
Learning from Human Feedback (RLHF) training process. It supports all
LLaMA model architectures, allowing for faster and cheaper training
and inference compared to the original ChatGPT. 

VOXFORMER: A CUTTING-EDGE BASELINE FOR 3D SEMANTIC OCCUPANCY
PREDICTION (GITHUB REPO)
[https://github.com/NVlabs/VoxFormer?utm_source=tldrai] 

The authors propose voxformer, a framework proposed to enable AI
systems to imagine the complete 3D geometry of occluded objects and
scenes from 2D images. VoxFormer uses a two-stage design where a
sparse set of visible and occupied voxel queries from depth estimation
is followed by a densification stage that generates dense 3D voxels.
The framework adopts a masked autoencoder design to propagate the
information to all the voxels by self-attention. Experiments on
SemanticKITTI show that VoxFormer outperforms the state of the art
with a relative improvement of 20.0% in geometry and 18.1% in
semantics and reduces GPU memory during training by ~45% to less than
16GB. 

üéÅ 

MISCELLANEOUS

40 YEARS OF AI COMPUTE (18 MINUTE READ)
[https://www.laconic.fi/ai-compute/?utm_source=tldrai] 

A nice compilation of various trends across the years in AI. Required
computation doubles every 9 months while the number of parameters
doubles every 18 months. Hardware improvements may improve through
2031. It doesn‚Äôt make sense to train models longer than 15 months.
This is not exclusive to language but also includes vision and RL.
Interactive charts included. 

CALM DOWN, THERE IS NO CONSCIOUS AI (4 MINUTE READ)
[https://gizmodo.com/ai-chatbot-bing-chatgpt-there-is-no-conscious-ai-1850157657?utm_source=tldrai]


This article reminds us that despite Bing AI‚Äôs and ChatGPT‚Äôs
seemingly human responses, these chatbots are not conscious or
sentient. We have a long way to go before we reach artificial general
intelligence (AGI), which is what OpenAI originally set out to solve
and protect humanity against. 

‚ö° 

QUICK LINKS

PERSONALIZED AI CONTENT GENERATION (PRODUCT LAUNCH)
[https://www.typeface.ai/?utm_source=tldrai] 

New company that combines multimodal model creation for useful
enterprise uses. 

ASK SENECA (PRODUCT LAUNCH)
[https://seneca.dylancastillo.co/?utm_source=tldrai] 

Get life advice from a GPT3-based stoic philosopher based on Seneca. 

META FORMING AN AI PRODUCT TEAM (3 MINUTE READ)
[https://www.bloomberg.com/news/articles/2023-02-27/meta-to-form-ai-product-team-to-keep-up-with-chatbot-competition?utm_source=tldrai]


Meta is forming an AI product team to focus on adding generative AI
capabilities to WhatsApp, Messenger, and Instagram. 

LEARN PROMPTING (ONLINE COURSE)
[https://learnprompting.org/?utm_source=tldrai] 

Learn prompting is a free, open source course on prompt engineering. 

If you are in a hiring position, you may want to HIRE AI TALENT
THROUGH OUR FREE JOB BOARD [https://tldr.tech/employer/sign-up]. 

If your company is interested in reaching an audience of AI
decision-makers, researchers, and engineers, you may want to ADVERTISE
WITH US
[https://danni763618.typeform.com/to/VdUfrHpY?utm_source=newsletter]. 

If you have any comments or feedback, just respond to this email! 

Thanks for reading, 
Andrew Tan (@ANDREWZTAN [https://twitter.com/andrewztan]) & Andrew
Carr (@ANDREW_N_CARR [https://twitter.com/andrew_n_carr]) 

If you don't want to receive future editions of TLDR AI, please¬†click
here to unsubscribe
[https://actions.tldrnewsletter.com/unsubscribe?ep=1&l=eedf6b14-3de3-11ed-9a32-0241b9615763&lc=041b8714-96a1-11ed-9899-3729ef006681&p=0fb8cba2-b732-11ed-81a6-33c00cd07e37&pt=campaign&pv=4&spa=1677592818&t=1677593153&s=19ffcd7bd23750c6d6dcb707bf83c7857e904019db2f5ae09e4ee42f05ea25b8].


¬†