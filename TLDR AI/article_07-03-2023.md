# Articles TLDR AI 07-03-2023

The Universal Speech Model is a 2B parameter model trained on 12
million hours of speech and 28 billion text sentences.¬† 

Sign Up [https://tldr.tech/ai?utm_source=tldr]|Jobs
[https://tldr.tech/jobs]|Advertise
[https://danni763618.typeform.com/to/K4Gdz1?utm_source=tldrai&utm_medium=newsletter#newsletter=ai]|View
Online
[https://actions.tldrnewsletter.com/web-version?ep=1&lc=041b8714-96a1-11ed-9899-3729ef006681&p=424b1a76-bcbd-11ed-a41a-1bf065bca83a&pt=campaign&t=1678197969&s=6115c87794ceb4c2a92533b891a622239541082e3a5bc719d58290bc37ff2316]


		TLDR 

DAILY UPDATE 2023-03-07

üöÄ 

HEADLINES & LAUNCHES

GOOGLE‚ÄôS UNIVERSAL SPEECH MODEL (7 MINUTE READ)
[https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html?utm_source=tldrai]


The first result in Google‚Äôs 1,000 language initiative, the
Universal Speech Model is a 2B parameter model trained on 12 million
hours of speech and 28 billion text sentences. It currently translates
300+ languages. This model is already in use at YouTube. The model is
an encoder-decoder architecture; the decoder is a time-variant
Transformer and the encoder is a Conformer model. The input is given
as log-mel spectrograms. It outperforms OpenAI‚Äôs whisper large-v2
mode across 18 languages. 

AI IS BEING USED TO DETECT BREAST CANCER (7 MINUTE READ)
[https://www.nytimes.com/2023/03/05/technology/artificial-intelligence-breast-cancer-detection.html?utm_source=tldrai]


Advancements in AI are beginning to deliver breakthroughs in breast
cancer screening by detecting the signs that doctors miss. So far, the
technology is showing an impressive ability to spot cancer at least as
well as human radiologists, according to early results and
radiologists, in what is one of the most tangible signs to date of how
AI can improve public health. Hungary has become a major testing
ground for AI software to spot cancer, as doctors debate whether the
technology will replace them in medical jobs. 

MICROSOFT LAUNCHES COPILOT (4 MINUTE READ)
[https://archive.ph/T0IJc?utm_source=tldrai] 

Microsoft, having brought artificial intelligence to its battle with
Google over search, is now turning to the latest AI technology to
catch up with rivals in the corporate applications market such as
Oracle, Salesforce, and SAP. Microsoft‚Äôs new AI ‚Äúcopilot‚Äù will
help answer customer calls, summarize sales meetings, and generate
marketing pitches. 

üß† 

RESEARCH & INNOVATION

HUMAN MOTION DIFFUSION AS A GENERATIVE PRIOR (4 MINUTE READ)
[https://priormdm.github.io/priorMDM-page/?utm_source=tldrai] 

The paper proposes a method to address the low availability of data in
motion generation tasks by using a pre-trained diffusion-based model
as a generative prior. The method includes a zero-shot setting that
generates long animations with controlled transitions, a few-shot
setting that generates two-person interaction, and a fine-tuning
setting that enables fine-grained control and editing. The proposed
method outperforms state-of-the-art approaches in quality scores and
interaction levels in a user study. 

IN-CONTEXT INSTRUCTION LEARNING (20 MINUTE READ)
[https://arxiv.org/abs/2302.14691?utm_source=tldrai] 

The paper presents a new approach to instruction learning called
In-Context Instruction Learning (ICIL) that significantly improves the
zero-shot task generalization performance of Large Language Models
(LLMs). While previous approaches to instruction learning have been
predominantly fine-tuning based, ICIL uses a single fixed prompt to
evaluate all tasks, resulting in a 9.3% improvement in performance for
the most powerful instruction-fine-tuned baseline. ICIL is shown to be
complementary to instruction-based fine-tuning, making it a promising
approach for improving zero-shot task generalization for LLMs. 

STYLIZE YOUR FACE IN ONLY ONE-SHOT (15 MINUTE READ)
[https://arxiv.org/abs/2303.03231?utm_source=tldrai] 

The paper presents StyO, a new model for one-shot face stylization
that combines content and style attributes from input images using
disentanglement and recombination strategies. The model uses latent
diffusion models and consists of two modules: Identifier
Disentanglement Learner (IDL) and Fine-grained Content Controller
(FCC). Evaluation results show that StyO outperforms current
state-of-the-art methods in producing high-quality stylized face
images. 

üßë‚Äçüíª 

ENGINEERING & RESOURCES

FASTER MODEL INIT ON GPU (7 MINUTE READ)
[http://lernapparat.de/faster-model-init?utm_source=tldrai] 

Loading language models to GPUs can be painfully slow, potentially
minutes long. This is because they tend to load all the weights onto
the CPU before transferring them to the GPU. Instead of this process,
you can write an initialization context manager which puts the weights
directly on the GPU - leading to pretty dramatic savings. 

AN OPEN-SOURCE FRAMEWORK FOR IN-CONTEXT LEARNING (GITHUB REPO)
[https://github.com/Shark-NLP/OpenICL?utm_source=tldrai] 

The authors introduce OpenICL, an open-source toolkit that facilitates
In-Context Learning (ICL) and Large Language Model (LLM) evaluation.
ICL is a new paradigm for LLM evaluation that adapts pre-trained
models to unseen tasks without parameter updates. OpenICL provides a
flexible architecture that users can easily combine with different
components to meet their requirements. The toolkit also includes
various state-of-the-art retrieval and inference methods, making it an
efficient and robust tool for LLM evaluation on a wide range of
Natural Language Processing (NLP) tasks. 

WORDS AS IMAGE - SEMANTIC TYPOGRAPHY (6 MINUTE READ)
[https://wordasimage.github.io/Word-As-Image-Page/?utm_source=tldrai] 

Semantic Typography is where letters in a word are shaped in such a
way to reflect the meaning of the word. So the A in Paris might look
like the Eiffel tower. This is typically a very labor intensive
process. However, using a differentiable renderer and a strong
text-to-image prior, it is now feasible to automate some of this
process. A delightful piece of work with fun visual examples included.


üéÅ 

MISCELLANEOUS

THE WALUIGI EFFECT (20 MINUTE READ)
[https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post?utm_source=tldrai]


After you train an LLM to satisfy a desirable property P, then it is
easier to elicit the LLM to satisfy the exact opposite property of P.
This means that it is often easy for a model to imitate a strong
negative agent even when the desired performance is that of a strong
positive agent. This post outlines why that may be and explores other
interesting problems encountered in LLMs. 

RECREATING FLAMINGO - RETROSPECTIVE (5 MINUTE READ)
[https://docs.google.com/document/d/1ZNGyVWYFUbzV0xuei4SED2QAakGjMpaaQALcKYQm46U/edit#heading=h.66l93vag25o8?utm_source=tldrai]


The main point of this post is exploring how to overcome loss
divergences even at small model scales (1 - 2 billion parameters) when
training on a large multimodal corpus. There are lots of interesting
tidbits about practical normalization and regularization schemes for
training these models. 

DEEP LEARNING COURSE (ONLINE COURSE)
[https://github.com/glouppe/info8010-deep-learning?utm_source=tldrai] 

This GitHub repo is a full course on Deep Learning for Spring 2023
from the University of Li√®ge. 

‚ö° 

QUICK LINKS

THOUSANDS SCAMMED BY AI VOICES (2 MINUTE READ)
[https://arstechnica.com/tech-policy/2023/03/rising-scams-use-ai-to-mimic-voices-of-loved-ones-in-financial-distress/?utm_source=tldrai]


In 2022, bad actors were able to steal $11 million by using
AI-generated voices to impersonate loved ones in emergencies. 

A DEEP DIVE INTO THE AI OBJECTIVE INSTITUTE (10 MINUTE READ)
[https://archive.ph/lViGi?utm_source=tldrai] 

Inspired by digital-privacy pioneer Peter Eckersley, the AI Objective
Institute is working toward guiding AI toward ‚Äúhuman flourishing‚Äù 

LLAMA INT8 (GITHUB REPO)
[https://github.com/tloen/llama-int8?utm_source=tldrai] 

LLaMA int8 is a fork of the LLaMA code that runs LLaMA-13B comfortably
within 24 GiB of RAM. 

If you are in a hiring position, you may want to HIRE AI TALENT
THROUGH OUR FREE JOB BOARD [https://tldr.tech/employer/sign-up]. 

If your company is interested in reaching an audience of AI
decision-makers, researchers, and engineers, you may want to ADVERTISE
WITH US
[https://danni763618.typeform.com/to/K4Gdz1?utm_source=tldrai&utm_medium=newsletter#newsletter=ai].


If you have any comments or feedback, just respond to this email! 

Thanks for reading, 
Andrew Tan (@ANDREWZTAN [https://twitter.com/andrewztan]) & Andrew
Carr (@ANDREW_N_CARR [https://twitter.com/andrew_n_carr]) 

If you don't want to receive future editions of TLDR AI, please¬†click
here to unsubscribe
[https://actions.tldrnewsletter.com/unsubscribe?ep=1&l=eedf6b14-3de3-11ed-9a32-0241b9615763&lc=041b8714-96a1-11ed-9899-3729ef006681&p=424b1a76-bcbd-11ed-a41a-1bf065bca83a&pt=campaign&pv=4&spa=1678197617&t=1678197969&s=61784368809fdc3e54e9a0954ca2e7de139c9533b1814bf4c8dd4aee1f855dcd].


¬†