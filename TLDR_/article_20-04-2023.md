# Articles TLDR  20-04-2023

## Article 1
### [Next level text-to-video synthesis (4 minute read)](https://tldr.tech)
### Summary 
 Next level text-to-video synthesis (4 minute read)

The recent progress in text to images was helped by using E2809ClatentE2809D diffusion where synthesis happens in a smaller space. This is cheaper while still maintaining high quality generations. It turns out with a few small tweaks this can also be applied to video generation. By making sure the latent vectors are all related through time, you can generate some amazingly consistent scenes.

## Article 2
### [The makers of stable diffusion trained some language models (HuggingFace)](https://tldr.tech)
### Summary 
 The makers of stable diffusion trained some language models (HuggingFace)

Trained for approximately 800B tokens now, with the goal to reach 1.5T tokens, Stability AIE28099s new StableLM models are 3B and 7B parameters and have a context length of 4k tokens. They are permissively licensed and available for commercial use. They were trained on a modified version of The Pile which will be released soon along with checkpoints that are further along. They have E2809CtunedE2809D versions of these models which use a variety of conversational and instruction datasets. Interestingly, these tuned versions have system prompts, which makes them one of the first open models to allow that sort of steering.

## Article 3
### [Transformer Math (8 minute read)](https://tldr.tech)
### Summary 
 Transformer Math (8 minute read)

How many GPUs do you need? How wide should your model be? These are regularly asked questions with fairly simple calculations. This post walks through these and a bunch more interesting questions involving Transformers and the math needed to train good ones. One interesting take away is that models should not be trained for less than 200B tokens regardless of size.

## Article 4
### [Visual Instruction Tuning (3 minute read)](https://tldr.tech)
### Summary 
 Visual Instruction Tuning (3 minute read)

The paper proposes using machine-generated instruction-following data to instruction tune large language models (LLMs) for multimodal tasks. This resulted in LLaVA, a large multimodal model that combines a vision encoder and language-only GPT-4. LLaVA exhibits impressive multimodal chat abilities and achieved a new state-of-the-art accuracy of 92.53% when fine-tuned on Science QA in synergy with GPT-4.

## Article 5
### [Low-code LLM: Visual Programming over LLMs (12 minute read)](https://tldr.tech)
### Summary 
 Low-code LLM: Visual Programming over LLMs (12 minute read)

The paper proposes Low-code LLM, a human-LLM interaction framework that uses low-code visual programming interactions to achieve more controllable responses for complex tasks. Low-code LLM includes a Planning LLM and an Executing LLM and offers controllable generation results and user-friendly interaction. The approach is demonstrated through four applications.

## Article 6
### [Promptr (GitHub Repo)](https://tldr.tech)
### Summary 
 Promptr (GitHub Repo)

Promptr is a CLI tool for operating on your codebase using GPT. Promptr dynamically includes one or more files into your GPT prompts, and it can optionally parse and apply the changes that GPT suggests to your codebase. Several prompt templates are included for various purposes, and users can create their own templates.

## Article 7
### [CLAP model weights released (GitHub Repo)](https://tldr.tech)
### Summary 
 CLAP model weights released (GitHub Repo)

Contrastive Language Image Pretraining (CLIP) is a model driving much of the recent progress in image synthetics and understanding. A similar Audio model CLAP was proposed recently and they just released a set of pretrained model weights.

## Article 8
### [Lift3D: Synthesize 3D Training Data by Lifting 2D GAN to 3D Generative Radiance Field (GitHub Repo)](https://tldr.tech)
### Summary 
 Lift3D: Synthesize 3D Training Data by Lifting 2D GAN to 3D Generative Radiance Field (GitHub Repo)

Lift3D, an inverted 2D-to-3D generation framework, addresses the limitations of NeRF-based 3D GANs by offering photorealistic output with adaptable resolution and accurate 3D annotations for downstream tasks. The framework is evaluated on autonomous driving datasets, demonstrating its ability to enhance the performance of 3D object detectors.

## Article 9
### [John Schulman Lecture: Reinforcement Learning from Human Feedback (Video)](https://tldr.tech)
### Summary 
 John Schulman Lecture: Reinforcement Learning from Human Feedback (Video)

Berkeley alum, OpenAI cofounder, and Chief Architect of ChatGPT John Schulman gives a lecture on the Reinforcement Learning from Human Feedback work powering ChatGPT. Schulman says truthfulness is one of the biggest problems in AI, and it turns out that reinforcement learning is one of the biggest ways to fix it.

## Article 10
### [Atlassian Releases AI Tools (1 minute read)](https://tldr.tech)
### Summary 
 Atlassian Releases AI Tools (1 minute read)

Atlassian Corp. unveiled new artificial intelligence features for its workplace collaboration tools, making it the latest technology company to utilize OpenAIE28099s generative AI models.

## Article 11
### [Inside The AI Talent Wars (3 minute read)](https://tldr.tech)
### Summary 
 Inside The AI Talent Wars (3 minute read)

Tech companies are now ransacking university AI programs in search of talent.

## Article 12
### [LLaVA (GitHub Repo)](https://tldr.tech)
### Summary 
 LLaVA (GitHub Repo)

LLaVA provides visual instruction tuning towards large language and vision models with GPT-4 level capabilities.

