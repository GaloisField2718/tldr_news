# Articles TLDR  07-03-2023

## Article 1
### [GoogleE28099s Universal Speech Model (7 minute read)](https://tldr.tech)
### Summary 
 GoogleE28099s Universal Speech Model (7 minute read)

The first result in GoogleE28099s 1,000 language initiative, the Universal Speech Model is a 2B parameter model trained on 12 million hours of speech and 28 billion text sentences. It currently translates 300+ languages. This model is already in use at YouTube. The model is an encoder-decoder architecture; the decoder is a time-variant Transformer and the encoder is a Conformer model. The input is given as log-mel spectrograms. It outperforms OpenAIE28099s whisper large-v2 mode across 18 languages.

## Article 2
### [AI is being used to detect breast cancer (7 minute read)](https://tldr.tech)
### Summary 
 AI is being used to detect breast cancer (7 minute read)

Advancements in AI are beginning to deliver breakthroughs in breast cancer screening by detecting the signs that doctors miss. So far, the technology is showing an impressive ability to spot cancer at least as well as human radiologists, according to early results and radiologists, in what is one of the most tangible signs to date of how AI can improve public health. Hungary has become a major testing ground for AI software to spot cancer, as doctors debate whether the technology will replace them in medical jobs.

## Article 3
### [Microsoft launches Copilot (4 minute read)](https://tldr.tech)
### Summary 
 Microsoft launches Copilot (4 minute read)

Microsoft, having brought artificial intelligence to its battle with Google over search, is now turning to the latest AI technology to catch up with rivals in the corporate applications market such as Oracle, Salesforce, and SAP. MicrosoftE28099s new AI E2809CcopilotE2809D will help answer customer calls, summarize sales meetings, and generate marketing pitches.

## Article 4
### [Human Motion Diffusion as a Generative Prior (4 minute read)](https://tldr.tech)
### Summary 
 Human Motion Diffusion as a Generative Prior (4 minute read)

The paper proposes a method to address the low availability of data in motion generation tasks by using a pre-trained diffusion-based model as a generative prior. The method includes a zero-shot setting that generates long animations with controlled transitions, a few-shot setting that generates two-person interaction, and a fine-tuning setting that enables fine-grained control and editing. The proposed method outperforms state-of-the-art approaches in quality scores and interaction levels in a user study.

## Article 5
### [In-Context Instruction Learning (20 minute read)](https://tldr.tech)
### Summary 
 In-Context Instruction Learning (20 minute read)

The paper presents a new approach to instruction learning called In-Context Instruction Learning (ICIL) that significantly improves the zero-shot task generalization performance of Large Language Models (LLMs). While previous approaches to instruction learning have been predominantly fine-tuning based, ICIL uses a single fixed prompt to evaluate all tasks, resulting in a 9.3% improvement in performance for the most powerful instruction-fine-tuned baseline. ICIL is shown to be complementary to instruction-based fine-tuning, making it a promising approach for improving zero-shot task generalization for LLMs.

## Article 6
### [Stylize Your Face in Only One-Shot (15 minute read)](https://tldr.tech)
### Summary 
 Stylize Your Face in Only One-Shot (15 minute read)

The paper presents StyO, a new model for one-shot face stylization that combines content and style attributes from input images using disentanglement and recombination strategies. The model uses latent diffusion models and consists of two modules: Identifier Disentanglement Learner (IDL) and Fine-grained Content Controller (FCC). Evaluation results show that StyO outperforms current state-of-the-art methods in producing high-quality stylized face images.

## Article 7
### [Faster model init on GPU (7 minute read)](https://tldr.tech)
### Summary 
 Faster model init on GPU (7 minute read)

Loading language models to GPUs can be painfully slow, potentially minutes long. This is because they tend to load all the weights onto the CPU before transferring them to the GPU. Instead of this process, you can write an initialization context manager which puts the weights directly on the GPU - leading to pretty dramatic savings.

## Article 8
### [An Open-Source Framework for In-context Learning (Github Repo)](https://tldr.tech)
### Summary 
 An Open-Source Framework for In-context Learning (Github Repo)

The authors introduce OpenICL, an open-source toolkit that facilitates In-Context Learning (ICL) and Large Language Model (LLM) evaluation. ICL is a new paradigm for LLM evaluation that adapts pre-trained models to unseen tasks without parameter updates. OpenICL provides a flexible architecture that users can easily combine with different components to meet their requirements. The toolkit also includes various state-of-the-art retrieval and inference methods, making it an efficient and robust tool for LLM evaluation on a wide range of Natural Language Processing (NLP) tasks.

## Article 9
### [Words as Image - Semantic Typography (6 minute read)](https://tldr.tech)
### Summary 
 Words as Image - Semantic Typography (6 minute read)

Semantic Typography is where letters in a word are shaped in such a way to reflect the meaning of the word. So the A in Paris might look like the Eiffel tower. This is typically a very labor intensive process. However, using a differentiable renderer and a strong text-to-image prior, it is now feasible to automate some of this process. A delightful piece of work with fun visual examples included.

## Article 10
### [The Waluigi Effect (20 minute read)](https://tldr.tech)
### Summary 
 The Waluigi Effect (20 minute read)

After you train an LLM to satisfy a desirable property P, then it is easier to elicit the LLM to satisfy the exact opposite property of P. This means that it is often easy for a model to imitate a strong negative agent even when the desired performance is that of a strong positive agent. This post outlines why that may be and explores other interesting problems encountered in LLMs.

## Article 11
### [Recreating Flamingo - retrospective (5 minute read)](https://tldr.tech)
### Summary 
 Recreating Flamingo - retrospective (5 minute read)

The main point of this post is exploring how to overcome loss divergences even at small model scales (1 - 2 billion parameters) when training on a large multimodal corpus. There are lots of interesting tidbits about practical normalization and regularization schemes for training these models.

## Article 12
### [Deep Learning Course (Online Course)](https://tldr.tech)
### Summary 
 Deep Learning Course (Online Course)

This GitHub repo is a full course on Deep Learning for Spring 2023 from the University of LiC3A8ge.

## Article 13
### [Thousands Scammed By AI Voices (2 minute read)](https://tldr.tech)
### Summary 
 Thousands Scammed By AI Voices (2 minute read)

In 2022, bad actors were able to steal $11 million by using AI-generated voices to impersonate loved ones in emergencies.

## Article 14
### [A Deep Dive Into The AI Objective Institute (10 minute read)](https://tldr.tech)
### Summary 
 A Deep Dive Into The AI Objective Institute (10 minute read)

Inspired by digital-privacy pioneer Peter Eckersley, the AI Objective Institute is working toward guiding AI toward E2809Chuman flourishingE2809D

