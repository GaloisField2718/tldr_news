# Articles TLDR  08-03-2023

## Article 1
### [Salesforce $250mm generative AI fund (2 minute read)](https://tldr.tech)
### Summary 
 Salesforce $250mm generative AI fund (2 minute read)

Initially investing in Cohere, Anthropic, You.com, and HealthAI this new fund will focus on accelerating generative AI companies across the board. They want to work with companies that will impact a user's daily workflow.

## Article 2
### [Stability AI acquires ClipDrop (1 minute read)</strong>](https://tldr.tech)
### Summary 
 Stability AI acquires ClipDrop (1 minute read)</strong>

Started in 2020, ClipDrop is a fantastic computer vision application that allows you to edit images with various AI enhanced tools. They plan to integrate future Stability models natively into ClipDrop which already has over 15 million users.

## Article 3
### [State of competitive ML 2022 (14 minute read)](https://tldr.tech)
### Summary 
 State of competitive ML 2022 (14 minute read)

Competitive ML is an ever growing field it is often hard to keep up, here are some highlights from 2022. Kaggle is still the most dominant platform for data science competitions, but there are other platforms with decent prize money. Python is the most commonly used language and 96% of Deep Learning solutions used PyTorch - which is up from 77% the prior year. Winners of NLP competitions used Transformers, while computer vision solutions mainly used CNNs. Tabular data competitions were mostly won by GBDTs, with LightGBM being the most popular. Half of the competition winners are first-time winners.

## Article 4
### [PALM-e (4 minute read)](https://tldr.tech)
### Summary 
 PALM-e (4 minute read)

The term E2809Cpositive transferE2809D is somewhat of a holy grail for large scale models. It means that when you train on two modalities, you get better performance than you would have training on just one or the other. This paper from Google shows that training on robot controls, video, and text shows positive transfer between domains. This is hypothesized to be a function of model scale, and this model is an impressive 562 billion parameters.

## Article 5
### [Prismer: A Vision-Language Model with Multi-Modal Experts (5 minute read)](https://tldr.tech)
### Summary 
 Prismer: A Vision-Language Model with Multi-Modal Experts (5 minute read)

The article discusses the high computational and data requirements of large pre-trained models used in vision-language learning, which require massive amounts of image-text data to be trained. The proposed alternative approach involves using separate sub-networks or "experts" for specific tasks, allowing for the use of domain-specific data and architectures that would not be feasible with a single large network. The proposed model, Prismer, is a visually conditioned autoregressive text generation model that utilizes powerful vision-only and language-only models along with modality-specific vision experts for open-ended vision-language reasoning tasks. The expert models are pre-trained and frozen, and connected through lightweight trainable components, comprising only 20% of total network parameters, leading to improved training efficiency and scalability.</span>

## Article 6
### [Davinci Functions (GitHub repo)](https://tldr.tech)
### Summary 
 Davinci Functions (GitHub repo)

There are common things you want to do with large language models such as text-divinci-003 such as write a list, answer a question, or verify a statement. This repo provides some simple tools for doing those tasks. It is a cool step towards building an abstraction layer on top of these models.

## Article 7
### [Code for the current best Attention-Free long context language model (GitHub repo)](https://tldr.tech)
### Summary 
 Code for the current best Attention-Free long context language model (GitHub repo)

The SSM literature has been receiving increased attention recently after S4 showed phenomenal performance on the long range arena benchmark. The latest work in the series has closed the gap on language modeling tasks with attention based Transformers while scaling to 10x longer context 100x faster.

## Article 8
### [Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene Representation from 2D Supervision (5 minute read)](https://tldr.tech)
### Summary 
 Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene Representation from 2D Supervision (5 minute read)

The paper introduces "Nerflets," a local neural radiance field that represents a 3D scene efficiently and with structure awareness. Nerflets are composed of multiple neural radiance fields that maintain their own spatial position, orientation, and extent to contribute to panoptic, density, and radiance reconstructions. By using only photometric and inferred panoptic image supervision, Nerflets allow the extraction of panoptic and photometric renderings from arbitrary views, and enable tasks rare for NeRFs, such as 3D panoptic segmentation and interactive editing. The experiments show that Nerflets fit and approximate the scene more efficiently than traditional global NeRFs.

## Article 9
### [Worried about AI stealing your job? Stay one step ahead with bite-sized interactive lessons (Sponsor)](https://tldr.tech)
### Summary 
 Worried about AI stealing your job? Stay one step ahead with bite-sized interactive lessons (Sponsor)

ItE28099s pretty clear that AI is going to eat the world. Are you ready for it? With Brilliant, you can master and practice AI concepts in just 15 minutes a day. Skill up on math, AI, neural networks, and more through guided interactive problem solving that's effective and fun. Try Brilliant free for 30 days + claim a 20% discount for TLDR readers

## Article 10
### [SGD in SQL (13 minute read)](https://tldr.tech)
### Summary 
 SGD in SQL (13 minute read)

Looking to reduce MLOps complexity? Just write everything in your database! This fun little post takes this idea to the limit and writes gradient calculations in SQL.

## Article 11
### [Scale AI: Why Data Will Power the AI Revolution (8 minute read)](https://tldr.tech)
### Summary 
 Scale AI: Why Data Will Power the AI Revolution (8 minute read)

This article by Index Ventures covers the story of Scale AI, one of the most successful AI startups from the last 5 years. Scale AI is the data platform for AI, providing high quality training data for leading machine learning teams. It was started by Alexandr Wang when he was just 19 years old and is now valued at over $7 billion. The article talks about how Scale got started, what made it so successful, and where the company and AI as a whole is headed.

## Article 12
### [Considerations For AI-Native Startups (7 minute read)](https://tldr.tech)
### Summary 
 Considerations For AI-Native Startups (7 minute read)

As large language models have exploded over the past year, numerous startups have begun to build AI-native applications to disrupt industries. There's been a lot of talk about how many of them are simply E2809Cwrappers on OpenAIE2809D as part of discussions about moats and defensibility. This article dives into how AI-native startups should think about moats, defensibility, and more.

## Article 13
### [DonE28099t Fear An AI-Induced Jobs Apocalypse Just Yet (8 minute read)](https://tldr.tech)
### Summary 
 DonE28099t Fear An AI-Induced Jobs Apocalypse Just Yet (8 minute read)

This article makes the case that because the west suffers from too little automation, not too much, it is unlikely that AI causes mass unemployment.

